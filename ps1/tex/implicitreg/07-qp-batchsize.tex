\item\subquestionpoints{6}\textbf{} \textbf{Coding question: implicit regularization of batch size}

We still work with the setup in part (e). For this sub-question, we use the same dataset and starter code as in sub-question (f). We will show that introducing noise in the training process also induces implicit regularization. In particular, we will observe how noise introduced by \emph{stochastic} gradient descent (SGD) helps generalization. The gradient descent algorithm we have covered so far in lecture is also known as \emph{batch} gradient descent, in which we look at the entire training set before taking a single update step. In SGD, we only look at a subset of the training examples before taking an update step. (Note that ``true" SGD looks at a single training example before taking an update step. In this problem, we will also be considering the generalization of SGD -- mini-batch gradient descent -- to generate more empirical evidence for your final observation!) SGD will be covered in more detail later in the course, but this is all you need to know to solve this problem.

\textbf{Implement} the SGD algorithm, and \textbf{plot} the training and test errors with mini-batch sizes $\{1, 5, 40\}$, learning rate $0.08$, and initialization $\alpha=0.1$. Similarly, use the number of gradient steps as $x$-axis, and training/test error as $y$-axis. For simplicity, the code for selecting a batch of examples is already provided in the starter code.
\textbf{Compare} the results with those in sub-question (g) with the same initialization. Does SGD find a better solution?

Your plot is expected to show that the stochasticity in the training process is also an important factor in the generalization performance --- in our setting, SGD finds a solution that generalizes better. In fact, a conjecture is that stochasticity in the optimization process (such as the noise introduced by a small batch size) helps the optimizer to find a solution that generalizes better. This conjecture can be proved in some simplified cases, such as the quadratically parameterized model in this sub-question (adapted from the paper \href{https://arxiv.org/abs/2006.08680}{HaoChen et al., 2020}), and can be observed empirically in many other cases.