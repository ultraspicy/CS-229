\textbf{Regarding Notation:} The notation used in this problem set matches the notation used in the lecture notes. Some notable differences from lecture notation: 
\begin{itemize}
    \item The superscript ``$(i)$" represents an index into the training set -- for example, $x^{(i)}$ is the $i$-th feature vector and $y^{(i)}$ is the $i$-th output variable. In lecture notation, these would instead be expressed as $x_i$ and $y_i$.

    \item The subscript $j$ represents an index in a vector -- in particular, $x^{(i)}_j$ represents the $j$-th feature in the feature vector $x^{(i)}$. In lecture notation, $x^{(i)}_j$ would be $h_j(x_i)$ or $x_i[j]$.

    \item The vector that contains the weights parameterizing a linear regression is expressed by the variable $\theta$, whereas lectures use the variable $\textbf{w}$. As such, $\theta_0 = w_0$, $\theta_1 = w_1$, and so on.

    \item The hypothesis function is expressed as $h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_d x_d = \theta^\top x$. In lecture notation, this would instead be $f_\textbf{w}(x) = w_0 h_0(x) + w_1 h_1(x) + \cdots + w_d h_d(x) = \textbf{w}^\top h(x)$. \begin{itemize}

        \item $x_0 = 1$ in the problem set notation, just as $h_0(x) = 1$ in the lecture notation.

    \end{itemize}
\end{itemize}

An overview of this notation is also given at the beginning of the lecture notes (pages 6-7).