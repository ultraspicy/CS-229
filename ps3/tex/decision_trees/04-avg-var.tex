\item \subquestionpoints{4} 

Bagging, short for "bootstrap aggregating," is a powerful ensemble learning technique that aims to improve the stability and accuracy of machine learning algorithms. It leverages the concept of bootstrapping, which involves simulating the drawing of a new sample from the true underlying distribution of the training set, as the training set is presumed to be a representative sample of the true distribution. In practice, this is done by generating new datasets through uniform sampling with replacement from the original dataset.

The "aggregating" component of bagging comes into play by repeating this bootstrapping process for each model in the ensemble, allowing each to be trained independently on a unique dataset. When considering decision trees, the method's utility becomes evident as it mitigates overfitting by ensuring that each tree in the ensemble is exposed to different subsets of the training data. This reduces the likelihood that the ensemble will fixate on particular data points, thus lowering overall variance. Statistically, each bootstrapped sample will contain, in expectation, about $1 - \frac{1}{e} \approx 63.2\%$ of unique data points from the original dataset.

However, the effectiveness of bagging depends on the characteristics of the underlying models. For models with low variance (and typically high bias), bagging may produce very similar models, which diminishes its benefits. On the other hand, with high-variance models such as decision trees, bagging capitalizes on the models' instability to promote diversity in the ensemble, thereby enhancing its performance. This results in an ensemble that maintains low bias while reducing variance, leading to a robust aggregate model.

Consider a training set X. In bootstrap sampling, each time we draw a random sample $Z$ of size N from the training data and obtain ${Z_1, Z_2, ..., Z_B}$ after $B$ times, i.e. we generate B different bootstrapped training data sets. If we apply bagging to regression trees, each time a tree $T_i (i = 1,2,...,B)$ is grown based on the bootstrapped data $Z_i$, and we average all the predictions to get:
\begin{align*}
    \hat{T(x)} =  \frac{1}{B}\sum_{i=1}^{B} T_i(x)
\end{align*}
Now, if $T_1, T_2,..., T_B$ is independent from each other, but each has the same variance $\sigma^2$, the variance of the average $\hat{T}$ is $\sigma^2/B$. However, in practice, the bagged trees could be similar to each other, resulting in correlated predictions. Assume $T_1, T_2,..., T_B$ still share the same variance $\sigma^2$, but have a positive pair-wise correlation $\rho$. We define the correlation between two random variables as:\\
\begin{align*}
    Corr(X,Y)=\dfrac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
\end{align*}
Thus, we have $\rho = Corr(T_i(x), T_j(x)), i \neq j$.

Show that in this case, the variance of the average is given by:
\begin{align*}
    Var(\frac{1}{B}\sum_{i=1}^{B} T_i(x)) = \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2
\end{align*}